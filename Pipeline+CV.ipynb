{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9852d114-3d87-4468-95f0-a09fbd2d9f15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sparkxgb\r\n  Downloading sparkxgb-0.1.tar.gz (3.6 kB)\r\n  Preparing metadata (setup.py) ... \u001B[?25l-\b \b\\\b \bdone\r\n\u001B[?25hCollecting pyspark==3.1.1\r\n  Downloading pyspark-3.1.1.tar.gz (212.3 MB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/212.3 MB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.1/212.3 MB\u001B[0m \u001B[31m3.8 MB/s\u001B[0m eta \u001B[36m0:00:57\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.5/212.3 MB\u001B[0m \u001B[31m7.9 MB/s\u001B[0m eta \u001B[36m0:00:27\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.0/212.3 MB\u001B[0m \u001B[31m9.6 MB/s\u001B[0m eta \u001B[36m0:00:22\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.7/212.3 MB\u001B[0m \u001B[31m12.5 MB/s\u001B[0m eta \u001B[36m0:00:17\u001B[0m\r\u001B[2K     \u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m2.7/212.3 MB\u001B[0m \u001B[31m15.6 MB/s\u001B[0m eta \u001B[36m0:00:14\u001B[0m\r\u001B[2K     \u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m4.2/212.3 MB\u001B[0m \u001B[31m20.0 MB/s\u001B[0m eta \u001B[36m0:00:11\u001B[0m\r\u001B[2K     \u001B[91m━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m6.1/212.3 MB\u001B[0m \u001B[31m24.4 MB/s\u001B[0m eta \u001B[36m0:00:09\u001B[0m\r\u001B[2K     \u001B[91m━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.1/212.3 MB\u001B[0m \u001B[31m28.4 MB/s\u001B[0m eta \u001B[36m0:00:08\u001B[0m\r\u001B[2K     \u001B[91m━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m10.9/212.3 MB\u001B[0m \u001B[31m41.8 MB/s\u001B[0m eta \u001B[36m0:00:05\u001B[0m\r\u001B[2K     \u001B[91m━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m13.8/212.3 MB\u001B[0m \u001B[31m62.2 MB/s\u001B[0m eta \u001B[36m0:00:04\u001B[0m\r\u001B[2K     \u001B[91m━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m16.4/212.3 MB\u001B[0m \u001B[31m71.7 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K     \u001B[91m━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m19.3/212.3 MB\u001B[0m \u001B[31m75.9 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K     \u001B[91m━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m22.5/212.3 MB\u001B[0m \u001B[31m79.1 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K     \u001B[91m━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m25.6/212.3 MB\u001B[0m \u001B[31m81.4 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K     \u001B[91m━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m29.0/212.3 MB\u001B[0m \u001B[31m87.5 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K     \u001B[91m━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m30.5/212.3 MB\u001B[0m \u001B[31m76.1 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m33.6/212.3 MB\u001B[0m \u001B[31m77.6 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m36.6/212.3 MB\u001B[0m \u001B[31m72.3 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m39.7/212.3 MB\u001B[0m \u001B[31m77.6 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m42.1/212.3 MB\u001B[0m \u001B[31m76.9 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m45.6/212.3 MB\u001B[0m \u001B[31m72.6 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m48.8/212.3 MB\u001B[0m \u001B[31m72.6 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m51.5/212.3 MB\u001B[0m \u001B[31m69.6 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m54.1/212.3 MB\u001B[0m \u001B[31m78.8 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m55.0/212.3 MB\u001B[0m \u001B[31m65.1 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m58.9/212.3 MB\u001B[0m \u001B[31m69.3 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m62.8/212.3 MB\u001B[0m \u001B[31m80.9 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m64.9/212.3 MB\u001B[0m \u001B[31m78.9 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m67.7/212.3 MB\u001B[0m \u001B[31m76.5 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m69.8/212.3 MB\u001B[0m \u001B[31m68.7 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m73.1/212.3 MB\u001B[0m \u001B[31m64.1 MB/s\u001B[0m eta \u001B[36m0:00:03\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m76.6/212.3 MB\u001B[0m \u001B[31m74.7 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.9/212.3 MB\u001B[0m \u001B[31m73.9 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m82.4/212.3 MB\u001B[0m \u001B[31m77.5 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m85.2/212.3 MB\u001B[0m \u001B[31m74.6 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m88.8/212.3 MB\u001B[0m \u001B[31m81.8 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m92.6/212.3 MB\u001B[0m \u001B[31m93.2 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m96.6/212.3 MB\u001B[0m \u001B[31m109.9 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m99.6/212.3 MB\u001B[0m \u001B[31m104.3 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m100.6/212.3 MB\u001B[0m \u001B[31m78.3 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m104.6/212.3 MB\u001B[0m \u001B[31m79.8 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m107.5/212.3 MB\u001B[0m \u001B[31m74.4 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m111.2/212.3 MB\u001B[0m \u001B[31m99.4 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m115.2/212.3 MB\u001B[0m \u001B[31m99.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m117.9/212.3 MB\u001B[0m \u001B[31m98.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━━━\u001B[0m \u001B[32m121.8/212.3 MB\u001B[0m \u001B[31m100.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━━\u001B[0m \u001B[32m125.9/212.3 MB\u001B[0m \u001B[31m101.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━\u001B[0m \u001B[32m129.8/212.3 MB\u001B[0m \u001B[31m113.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━\u001B[0m \u001B[32m131.6/212.3 MB\u001B[0m \u001B[31m91.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━━\u001B[0m \u001B[32m133.8/212.3 MB\u001B[0m \u001B[31m81.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m136.4/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m138.1/212.3 MB\u001B[0m \u001B[31m57.1 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━━\u001B[0m \u001B[32m138.9/212.3 MB\u001B[0m \u001B[31m49.2 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━━\u001B[0m \u001B[32m141.1/212.3 MB\u001B[0m \u001B[31m49.1 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━━━\u001B[0m \u001B[32m143.1/212.3 MB\u001B[0m \u001B[31m49.1 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━\u001B[0m \u001B[32m146.1/212.3 MB\u001B[0m \u001B[31m49.3 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━━\u001B[0m \u001B[32m147.8/212.3 MB\u001B[0m \u001B[31m50.2 MB/s\u001B[0m eta \u001B[36m0:00:02\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m151.2/212.3 MB\u001B[0m \u001B[31m68.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m153.8/212.3 MB\u001B[0m \u001B[31m79.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━━━\u001B[0m \u001B[32m154.9/212.3 MB\u001B[0m \u001B[31m62.5 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m158.7/212.3 MB\u001B[0m \u001B[31m75.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━━\u001B[0m \u001B[32m159.0/212.3 MB\u001B[0m \u001B[31m60.9 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━━\u001B[0m \u001B[32m162.6/212.3 MB\u001B[0m \u001B[31m58.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━━\u001B[0m \u001B[32m165.6/212.3 MB\u001B[0m \u001B[31m70.1 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m167.7/212.3 MB\u001B[0m \u001B[31m62.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━━━\u001B[0m \u001B[32m171.4/212.3 MB\u001B[0m \u001B[31m84.8 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━━\u001B[0m \u001B[32m175.3/212.3 MB\u001B[0m \u001B[31m85.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m179.0/212.3 MB\u001B[0m \u001B[31m106.7 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━━\u001B[0m \u001B[32m182.7/212.3 MB\u001B[0m \u001B[31m106.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m185.8/212.3 MB\u001B[0m \u001B[31m99.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━━\u001B[0m \u001B[32m187.5/212.3 MB\u001B[0m \u001B[31m82.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m190.6/212.3 MB\u001B[0m \u001B[31m78.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m192.5/212.3 MB\u001B[0m \u001B[31m68.4 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━━\u001B[0m \u001B[32m193.8/212.3 MB\u001B[0m \u001B[31m59.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m197.4/212.3 MB\u001B[0m \u001B[31m65.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━━\u001B[0m \u001B[32m201.1/212.3 MB\u001B[0m \u001B[31m73.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m\u001B[90m━\u001B[0m \u001B[32m204.4/212.3 MB\u001B[0m \u001B[31m99.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[90m╺\u001B[0m \u001B[32m208.0/212.3 MB\u001B[0m \u001B[31m99.6 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m210.1/212.3 MB\u001B[0m \u001B[31m84.0 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m211.5/212.3 MB\u001B[0m \u001B[31m72.2 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[91m╸\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m65.3 MB/s\u001B[0m eta \u001B[36m0:00:01\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m212.3/212.3 MB\u001B[0m \u001B[31m3.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25h  Preparing metadata (setup.py) ... \u001B[?25l-\b \b\\\b \bdone\r\n\u001B[?25hCollecting py4j==0.10.9\r\n  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\r\n\u001B[?25l     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m0.0/198.6 kB\u001B[0m \u001B[31m?\u001B[0m eta \u001B[36m-:--:--\u001B[0m\r\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m198.6/198.6 kB\u001B[0m \u001B[31m21.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n\u001B[?25hBuilding wheels for collected packages: sparkxgb, pyspark\r\n  Building wheel for sparkxgb (setup.py) ... \u001B[?25l-\b \b\\\b \b|\b \bdone\r\n\u001B[?25h  Created wheel for sparkxgb: filename=sparkxgb-0.1-py3-none-any.whl size=5630 sha256=88dbd3d1a94c52280a7e9860bf1e340c4a609f2575f40ec0e486b028be358cef\r\n  Stored in directory: /root/.cache/pip/wheels/b7/0c/a1/786408e13056fabeb8a72134e101b1e142fc95905c7b0e2a71\r\n  Building wheel for pyspark (setup.py) ... \u001B[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n\u001B[?25h  Created wheel for pyspark: filename=pyspark-3.1.1-py2.py3-none-any.whl size=212767590 sha256=ef70bbf13e1bdc16f6fb526112234a1cf0e1ada18ecb87f7cdd0ec223a527b04\r\n  Stored in directory: /root/.cache/pip/wheels/a0/3f/72/8efd988f9ae041f051c75e6834cd92dd6d13a726e206e8b6f3\r\nSuccessfully built sparkxgb pyspark\r\nInstalling collected packages: py4j, pyspark, sparkxgb\r\n\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\ndatabricks-feature-store 0.13.5 requires pyspark<4,>=3.1.2, but you have pyspark 3.1.1 which is incompatible.\u001B[0m\u001B[31m\r\n\u001B[0mSuccessfully installed py4j-0.10.9 pyspark-3.1.1 sparkxgb-0.1\r\n\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.2.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.2.1\u001B[0m\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sparkxgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fefbcf5-5167-4254-ab39-3aefed847df0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#imports and paths\n",
    "\n",
    "# General Packages:\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "from datetime import date, timedelta\n",
    "from pyspark.sql.functions import col, isnan, when, mean,avg, stddev, count, sum, countDistinct, min, round, max, year\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql import functions as F\n",
    "from collections import namedtuple\n",
    "\n",
    "# For Pipeline\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.sql import DataFrame, Row\n",
    "from pyspark.ml.linalg import Vectors,VectorUDT\n",
    "from pyspark.sql.functions import array, create_map, struct\n",
    "\n",
    "# For Feature Engineering\n",
    "from pyspark.ml.feature import VectorAssembler,MinMaxScaler,BucketedRandomProjectionLSH,VectorSlicer,IndexToString\n",
    "from pyspark.sql.types import FloatType, IntegerType, TimestampType, StringType\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.sql.functions import col, to_timestamp, hour, concat, date_format, rand, when,concat,substring,lit,udf,lower,sum as ps_sum,count as ps_count,row_number\n",
    "from pyspark.sql.window import *\n",
    "from pyspark.sql.window import Window\n",
    "from graphframes import GraphFrame\n",
    "\n",
    "# For Modeling:\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, DecisionTreeClassifier, MultilayerPerceptronClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from sparkxgb import XGBoostClassifier\n",
    "from pyspark.ml.tuning import ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import random\n",
    "\n",
    "#dbutils.fs.help()\n",
    "data_BASE_DIR = \"/mnt/\"\n",
    "#display(dbutils.fs.ls(f\"{data_BASE_DIR}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfa4a384-7930-4b2f-b885-fdf157f2a625",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "blob_container  = \"w261-container\"\n",
    "storage_account = \"whiskeythedog\"\n",
    "secret_scope    = \"whiskeythedog-scope\"                   \n",
    "secret_key      = \"whikeythedog-key\"  \n",
    "blob_url = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"\n",
    "\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net\",\n",
    "  dbutils.secrets.get(scope = secret_scope, key = secret_key)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "577a0a72-29f4-4d3e-a9e5-b10fecfcd243",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/mids-w261/HW5/</td><td>HW5/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/mids-w261/OTPW_12M/</td><td>OTPW_12M/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/mids-w261/OTPW_1D_CSV/</td><td>OTPW_1D_CSV/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/mids-w261/OTPW_36M/</td><td>OTPW_36M/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/mids-w261/OTPW_3M/</td><td>OTPW_3M/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/mids-w261/OTPW_3M_2015.csv</td><td>OTPW_3M_2015.csv</td><td>1500620247</td><td>1679772070000</td></tr><tr><td>dbfs:/mnt/mids-w261/OTPW_60M/</td><td>OTPW_60M/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/mids-w261/airport-codes_csv.csv</td><td>airport-codes_csv.csv</td><td>6232459</td><td>1677623514000</td></tr><tr><td>dbfs:/mnt/mids-w261/datasets_final_project/</td><td>datasets_final_project/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/mids-w261/datasets_final_project_2022/</td><td>datasets_final_project_2022/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/mids-w261/HW5/",
         "HW5/",
         0,
         0
        ],
        [
         "dbfs:/mnt/mids-w261/OTPW_12M/",
         "OTPW_12M/",
         0,
         0
        ],
        [
         "dbfs:/mnt/mids-w261/OTPW_1D_CSV/",
         "OTPW_1D_CSV/",
         0,
         0
        ],
        [
         "dbfs:/mnt/mids-w261/OTPW_36M/",
         "OTPW_36M/",
         0,
         0
        ],
        [
         "dbfs:/mnt/mids-w261/OTPW_3M/",
         "OTPW_3M/",
         0,
         0
        ],
        [
         "dbfs:/mnt/mids-w261/OTPW_3M_2015.csv",
         "OTPW_3M_2015.csv",
         1500620247,
         1679772070000
        ],
        [
         "dbfs:/mnt/mids-w261/OTPW_60M/",
         "OTPW_60M/",
         0,
         0
        ],
        [
         "dbfs:/mnt/mids-w261/airport-codes_csv.csv",
         "airport-codes_csv.csv",
         6232459,
         1677623514000
        ],
        [
         "dbfs:/mnt/mids-w261/datasets_final_project/",
         "datasets_final_project/",
         0,
         0
        ],
        [
         "dbfs:/mnt/mids-w261/datasets_final_project_2022/",
         "datasets_final_project_2022/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect the Mount's Final Project folder \n",
    "# Please IGNORE dbutils.fs.cp(\"/mnt/mids-w261/datasets_final_project/stations_data/\", \"/mnt/mids-w261/datasets_final_project_2022/stations_data/\", recurse=True)\n",
    "mids261_mount_path = \"dbfs:/mnt/mids-w261/\" #no permission to write to this mounted blob storage; \n",
    "display(dbutils.fs.ls(f\"{mids261_mount_path}\")) #3M, 12M, 36M, 60M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1be718f1-d10d-4c34-a754-684d11098386",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/</td><td>parquet_airlines_data/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data_1y/</td><td>parquet_airlines_data_1y/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data_3m/</td><td>parquet_airlines_data_3m/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data_6m/</td><td>parquet_airlines_data_6m/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data/</td><td>parquet_weather_data/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data_1y/</td><td>parquet_weather_data_1y/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data_3m/</td><td>parquet_weather_data_3m/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data_6m/</td><td>parquet_weather_data_6m/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/mnt/mids-w261/datasets_final_project_2022/stations_data/</td><td>stations_data/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data/",
         "parquet_airlines_data/",
         0,
         0
        ],
        [
         "dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data_1y/",
         "parquet_airlines_data_1y/",
         0,
         0
        ],
        [
         "dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data_3m/",
         "parquet_airlines_data_3m/",
         0,
         0
        ],
        [
         "dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_airlines_data_6m/",
         "parquet_airlines_data_6m/",
         0,
         0
        ],
        [
         "dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data/",
         "parquet_weather_data/",
         0,
         0
        ],
        [
         "dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data_1y/",
         "parquet_weather_data_1y/",
         0,
         0
        ],
        [
         "dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data_3m/",
         "parquet_weather_data_3m/",
         0,
         0
        ],
        [
         "dbfs:/mnt/mids-w261/datasets_final_project_2022/parquet_weather_data_6m/",
         "parquet_weather_data_6m/",
         0,
         0
        ],
        [
         "dbfs:/mnt/mids-w261/datasets_final_project_2022/stations_data/",
         "stations_data/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect the Mount's Final Project folder \n",
    "# Please IGNORE dbutils.fs.cp(\"/mnt/mids-w261/datasets_final_project/stations_data/\", \"/mnt/mids-w261/datasets_final_project_2022/stations_data/\", recurse=True)\n",
    "data_BASE_DIR = \"dbfs:/mnt/mids-w261/datasets_final_project_2022/\"\n",
    "display(dbutils.fs.ls(f\"{data_BASE_DIR}\"))\n",
    "#display(dbutils.fs.ls(f\"{data_BASE_DIR}stations_data/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f4b2a4c-d483-41bf-b757-43a34b5b7aca",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "491fb356-5e1c-40bb-88be-dacb1e8c1ec4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Load the 60 month joined dataset\n",
    "path_60m =[f\"{mids261_mount_path}OTPW_60M\"]\n",
    "OTPW_60M = spark.read.option(\"header\",True) \\\n",
    "                       .option(\"compression\", \"gzip\") \\\n",
    "                       .option(\"delimiter\",\",\") \\\n",
    "                       .csv(path_60m).cache()\n",
    "OTPW_60M.createOrReplaceTempView(\"OTPW_60M\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e81a8db7-6851-448e-9744-9b48a41e04ad",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Custom Transformation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c539bc0-b35b-4bbb-9faf-dd04117e389a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CustomFunctionTransformer(Transformer):\n",
    "    def __init__(self, custom_function):\n",
    "        self.custom_function = custom_function\n",
    "    \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        # Apply your custom function to the DataFrame\n",
    "        transformed_data = self.custom_function(df)\n",
    "        return transformed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "226c0656-0b6a-4636-b69b-895cf024fb32",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def data_cleansing(df):\n",
    "\n",
    "    # Drop duplicates\n",
    "    df = df.dropDuplicates()\n",
    "\n",
    "    # Feature Selection\n",
    "    short  = ['QUARTER','DAY_OF_MONTH','DAY_OF_WEEK','YEAR','MONTH','FL_DATE','OP_UNIQUE_CARRIER',\n",
    "    'ORIGIN',\n",
    "    'pagerank',\n",
    "    'inDegree',\n",
    "    'outDegree',\n",
    "    'DEST',\n",
    "    'DEP_TIME',\n",
    "    'DEP_DEL15',\n",
    "    'DISTANCE',\n",
    "    'origin_type',\n",
    "    'sched_depart_date_time_UTC',\n",
    "    'HourlyDewPointTemperature',\n",
    "    'HourlyDryBulbTemperature',\n",
    "    'HourlyPrecipitation',\n",
    "    'HourlyRelativeHumidity',\n",
    "    'HourlySeaLevelPressure',\n",
    "    'HourlyVisibility',\n",
    "    'HourlyWetBulbTemperature',\n",
    "    'HourlyWindDirection',\n",
    "    'HourlyWindSpeed']\n",
    "    df = df.select(short).persist()\n",
    "\n",
    "    # Cast numeric features to float \n",
    "    list_of_columns = ['DEP_DEL15', 'DISTANCE', 'HourlyDewPointTemperature', 'HourlyDryBulbTemperature','HourlyPrecipitation','HourlyRelativeHumidity','HourlySeaLevelPressure','pagerank',\n",
    "    'inDegree',\n",
    "    'outDegree']\n",
    "    for column_name in list_of_columns:\n",
    "        df = df.withColumn(column_name, df[column_name].cast(FloatType()))\n",
    "\n",
    "    list_of_columns = ['QUARTER','DAY_OF_MONTH','DAY_OF_WEEK', 'YEAR', 'MONTH']\n",
    "    for column_name in list_of_columns:\n",
    "        df = df.withColumn(column_name, df[column_name].cast(StringType()))\n",
    "\n",
    "    # Convert FL_Date to date:\n",
    "    data_col = ['FL_DATE']\n",
    "    for column_name in data_col:\n",
    "        df = df.withColumn(column_name, date_format(df[column_name], \"yyyy-MM-dd\"))\n",
    "\n",
    "    # Create hour_of_date column\n",
    "    df = df.withColumn(\"hour_of_day\", hour(col(\"sched_depart_date_time_UTC\")))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def get_mode(df, column_name):\n",
    "    mode_value = df.groupBy(column_name).count().orderBy(F.desc('count')).limit(1).collect()[0][0]\n",
    "    return mode_value\n",
    "\n",
    "# handle column and do imputation\n",
    "def summarize_column(df, column_name, outcome_col, sample_rate=1, null_method='del', to_show=True, seed=42):\n",
    "    df = df if sample_rate==1 else df.sample(False, sample_rate, seed)\n",
    "    # cast the specific column to float\n",
    "    df_with_cast = df.withColumn(column_name, col(column_name).cast('float'))\n",
    "    df_filtered = df_with_cast.filter(col(column_name).isNotNull())\n",
    "    \n",
    "    # determine flags for different conditions\n",
    "    df_with_flags = df_with_cast.withColumn('is_null_or_nan', col(column_name).isNull() | isnan(col(column_name)))\n",
    "    df_with_flags = df_with_flags.withColumn('is_string_nan_or_null', col(column_name).isin('nan', 'null'))\n",
    "    df_with_flags = df_with_flags.withColumn('is_float_castable', ~col('is_null_or_nan') & ~col('is_string_nan_or_null'))\n",
    "\n",
    "    agg_result = df_with_flags.agg(\n",
    "        count(col(column_name)).alias('count'),\n",
    "        sum(when(col('is_null_or_nan'), 1).otherwise(0)).alias('nulls_nans_count'),\n",
    "        sum(when(col('is_string_nan_or_null'), 1).otherwise(0)).alias('string_nans_nulls_count'),\n",
    "        sum(when(col('is_float_castable'), 1).otherwise(0)).alias('float_castable_count'),\n",
    "        sum(when(~col('is_float_castable'), 1).otherwise(0)).alias('uncastable_count'), # Assuming is_float_castable is a boolean column\n",
    "        min(col(column_name)).alias('min_value'),\n",
    "        max(col(column_name)).alias('max_value'),\n",
    "        round(avg(col(column_name)), 2).alias('mean'),\n",
    "        round(stddev(col(column_name)), 2).alias('std_dev')\n",
    "    )\n",
    "\n",
    "    # summarization values\n",
    "    col_mean = df_filtered.agg(mean(col(column_name)).alias('mean')).collect()[0]['mean']\n",
    "    col_mode = get_mode(df_filtered, column_name)\n",
    "    col_median = df_filtered.approxQuantile(column_name, [0.5], 0.01)[0]\n",
    "    print(col_mean, col_mode, col_median)\n",
    "\n",
    "    # plot histogram if to_show is True\n",
    "    if to_show:\n",
    "        display(agg_result.collect())\n",
    "\n",
    "        pandas_df = df_with_cast.select([column_name, outcome_col]).toPandas()\n",
    "\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        plt.subplot(1, 3, (1,2))\n",
    "\n",
    "        sns.histplot(data=pandas_df, x=column_name,  hue=outcome_col, bins=30, palette=['teal', 'rebeccapurple'],\n",
    "                            kde=False, linewidth=0.5, multiple=\"stack\", element=\"step\", edgecolor=\"none\")\n",
    "\n",
    "        col_mean = df_filtered.agg(mean(col(column_name)).alias('mean')).collect()[0]['mean']\n",
    "        col_median = df_filtered.approxQuantile(column_name, [0.5], 0.01)[0]\n",
    "        plt.axvline(x=col_mean,   color='red',    linestyle='--')\n",
    "        plt.axvline(x=col_median, color='pink',   linestyle='--')\n",
    "        plt.gca().set_facecolor('lightgray')\n",
    "        plt.title(column_name)\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "\n",
    "        res = df_with_flags.withColumn(f'{column_name}_0_null', when((col(column_name).isNull()) & (col(outcome_col) == 0), 1) .otherwise(0)) \\\n",
    "                        .withColumn(f'{column_name}_0_num', when((~col(column_name).isNull()) & (col(outcome_col) == 0), 1) .otherwise(0)) \\\n",
    "                        .withColumn(f'{column_name}_1_null', when((col(column_name).isNull()) & (col(outcome_col) == 1), 1) .otherwise(0)) \\\n",
    "                        .withColumn(f'{column_name}_1_num', when((~col(column_name).isNull()) & (col(outcome_col) == 1), 1) .otherwise(0))\n",
    "\n",
    "        aggy = res.agg(\n",
    "                        count(f'{column_name}_0_null').alias('count'),\n",
    "                        sum(f'{column_name}_0_null').alias('0_null'),\n",
    "                        sum(f'{column_name}_0_num').alias('0_num'),\n",
    "                        sum(f'{column_name}_1_null').alias('1_null'),\n",
    "                        sum(f'{column_name}_1_num').alias('1_num')\n",
    "                ).collect()\n",
    "\n",
    "        display(aggy)\n",
    "\n",
    "        bar_data = pd.DataFrame({\n",
    "            'Category': ['Float-Castable', 'Float-Castable', 'Uncastable', 'Uncastable'],\n",
    "            'Outcome': [0, 1, 0, 1],\n",
    "            'Count': [aggy[0][2], aggy[0][4], aggy[0][1], aggy[0][3]]\n",
    "        })\n",
    "\n",
    "        plt.gca().set_facecolor('lightgray')\n",
    "        sns.barplot(x='Category', y='Count', hue='Outcome', data=bar_data, palette=['teal', 'rebeccapurple'])\n",
    "        plt.title('Null Counts by Category')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "    # return only the castable ones\n",
    "    return agg_result\n",
    "\n",
    "def summarize_columns(df, cols_to_summarize, view=True, seed=42):\n",
    "    # summary results dataframe\n",
    "    result_df = None\n",
    "\n",
    "    # loop through the columns\n",
    "    for col_name in cols_to_summarize:\n",
    "        agg_result = summarize_column(df, col_name)\n",
    "        result_df = result_df.union(agg_result) if result_df is not None else agg_result\n",
    "\n",
    "    if view: \n",
    "        display(result_df)\n",
    "    return result_df\n",
    "\n",
    "outcome_col = 'DEP_DEL15'\n",
    "cols_to_summarize = ['HourlyDewPointTemperature',\n",
    "                'HourlyDryBulbTemperature',\n",
    "                'HourlyPrecipitation',\n",
    "                'HourlyRelativeHumidity',\n",
    "                'HourlySeaLevelPressure',\n",
    "                'HourlyVisibility',\n",
    "                'HourlyWetBulbTemperature',\n",
    "                'HourlyWindDirection',\n",
    "                'HourlyWindGustSpeed',\n",
    "                'HourlyWindSpeed',\n",
    "                'pagerank',\n",
    "                'inDegree',\n",
    "                'outDegree']\n",
    "\n",
    "\n",
    "def handle_missing_values(df, column_name, method='mean'):\n",
    "    # cast the column to float\n",
    "    df_casted = df.withColumn(column_name, col(column_name).cast('float'))\n",
    "    \n",
    "    # filter out the non-castable rows\n",
    "    df_filtered = df_casted.filter(col(column_name).isNotNull())\n",
    "    \n",
    "    # determine the replacement value based on the method\n",
    "    if method == 'mean':\n",
    "        repl_value = df_filtered.agg(mean(col(column_name))).collect()[0][0]\n",
    "    elif method == 'median':\n",
    "        repl_value = df_filtered.approxQuantile(column_name, [0.5], 0)[0]\n",
    "    elif method == 'mode':\n",
    "        mode_count = df_filtered.groupBy(column_name).count().orderBy(F.desc('count'))\n",
    "        repl_value = mode_count.collect()[0][0]\n",
    "\n",
    "    # Fill the missing values with the replacement value\n",
    "    df_filled = df_casted.withColumn(\n",
    "        column_name, when(col(column_name).isNotNull(), col(column_name)).otherwise(repl_value)\n",
    "    )\n",
    "    \n",
    "    return df_filled\n",
    "\n",
    "def null_handling(df):\n",
    "    null_cols =  [('HourlyDewPointTemperature', 'median'),\n",
    "                    ('HourlyDryBulbTemperature', 'median'),\n",
    "                    ('HourlyPrecipitation', 'median'),\n",
    "                    ('HourlyRelativeHumidity', 'median'),\n",
    "                    ('HourlySeaLevelPressure', 'median'),\n",
    "                    ('HourlyVisibility', 'median'),\n",
    "                    ('HourlyWetBulbTemperature', 'median'),\n",
    "                    ('HourlyWindDirection', 'median'),\n",
    "                    ('HourlyWindSpeed', 'median'),\n",
    "                    ('pagerank', 'median'),\n",
    "                    ('inDegree', 'median'),\n",
    "                    ('outDegree', 'median'),]\n",
    "\n",
    "    for column_name in null_cols:\n",
    "        print(column_name)\n",
    "        if isinstance(column_name, tuple): # if it's a tuple, then use the 1st element as method\n",
    "            df = handle_missing_values(df, column_name[0], method=column_name[1])\n",
    "        else:\n",
    "            df = df.withColumn(column_name, when(col(column_name) == \"null\", 0).otherwise(col(column_name).cast(\"float\")))\n",
    "\n",
    "    # Drop NA rows\n",
    "    df = df.na.drop()\n",
    "    return df\n",
    "\n",
    "def feature_engineering(df):\n",
    "    ##### Create new features:\n",
    "\n",
    "    ### Is Holiday\n",
    "    holiday_list = ['2015-01-01','2015-01-02','2015-01-07','2015-01-08','2015-01-09','2015-10-10','2015-10-11','2015-10-12','2015-11-10','2015-11-11','2015-11-12','2015-11-25','2015-11-26','2015-11-27','2015-11-28','2015-12-23','2015-12-24','2015-12-25','2015-12-26','2015-12-30','2015-12-31','2015-02-14','2015-02-15','2015-02-16','2015-02-17','2015-05-24','2015-05-25','2015-05-26','2015-07-03','2015-07-04','2015-07-05','2015-09-05','2015-09-06','2015-09-07','2016-01-01','2016-01-16','2016-01-17','2016-01-18','2016-01-02','2016-10-10','2016-10-08','2016-10-09','2016-11-10','2016-11-11','2016-11-12','2016-11-23','2016-11-24','2016-11-25','2016-11-26','2016-12-23','2016-12-24','2016-12-25','2016-12-26','2016-12-30','2016-12-31','2016-02-20','2016-02-21','2016-02-22','2016-05-28','2016-05-29','2016-05-30','2016-07-03','2016-07-04','2016-07-05','2016-09-03','2016-09-04','2016-09-05','2017-01-01','2017-01-02','2017-01-21','2017-01-22','2017-01-23','2017-10-07','2017-10-08','2017-10-09','2017-11-10','2017-11-11','2017-11-12','2017-11-22','2017-11-23','2017-11-24','2017-11-25','2017-12-23','2017-12-24','2017-12-25','2017-12-26','2017-12-30','2017-12-31','2017-02-18','2017-02-19','2017-02-20','2017-05-27','2017-05-28','2017-05-29','2017-07-03','2017-07-04','2017-07-05','2017-09-02','2017-09-03','2017-09-04','2018-01-01','2018-01-02','2018-01-20','2018-01-21','2018-01-22','2018-10-13','2018-10-14','2018-10-15','2018-11-10','2018-11-11','2018-11-12','2018-11-28','2018-11-29','2018-11-30','2018-12-01','2018-12-23','2018-12-24','2018-12-25','2018-12-26','2018-12-30','2018-12-31','2018-02-17','2018-02-18','2018-02-19','2018-05-26','2018-05-27','2018-05-28','2018-07-03','2018-07-04','2018-07-05','2018-09-01','2018-09-02','2018-09-03','2019-01-01','2019-01-02','2019-01-20','2019-01-21','2019-01-22','2019-10-12','2019-10-13','2019-10-14','2019-11-10','2019-11-11','2019-11-12','2019-11-27','2019-11-28','2019-11-29','2019-11-30','2019-12-23','2019-12-24','2019-12-25','2019-12-26','2019-12-30','2019-12-31','2019-02-16','2019-02-17','2019-02-18','2019-05-25','2019-05-26','2019-05-27','2019-07-03','2019-07-04','2019-07-05','2019-08-31','2019-09-01','2019-09-02','2020-01-01','2020-01-18','2020-01-19','2020-01-02','2020-01-20','2020-10-10','2020-10-11','2020-10-12','2020-11-10','2020-11-11','2020-11-12','2020-11-25','2020-11-26','2020-11-27','2020-11-28','2020-12-23','2020-12-24','2020-12-25','2020-12-26','2020-12-30','2020-12-31','2020-02-15','2020-02-16','2020-02-17','2020-05-23','2020-05-24','2020-05-25','2020-07-03','2020-07-04','2020-07-05','2020-09-05','2020-09-06','2020-09-07']\n",
    "\n",
    "    df = df.withColumn(\"is_holiday\", F.when(F.col(\"FL_DATE\").isin(holiday_list), 1).otherwise(0))\n",
    "\n",
    "\n",
    "################################################\n",
    "#### Want to BUCKET into (e.g. 8 hr windows) ###\n",
    "################################################\n",
    "\n",
    "    conditions = [\n",
    "        (F.hour(\"sched_depart_date_time_UTC\").isin([23, 0, 1, 2, 3, 4, 5, 6]), 1),\n",
    "        (F.hour(\"sched_depart_date_time_UTC\").isin([7, 8, 9, 10, 11, 12, 13, 14]), 2),\n",
    "        (F.hour(\"sched_depart_date_time_UTC\").isin([15, 16, 17, 18, 19, 20, 21, 22]), 3)]\n",
    "\n",
    "    df = df.withColumn(\"part_of_day\", F.coalesce(*[F.when(condition, value) for condition, value in conditions]))\n",
    "\n",
    "\n",
    "    ### % of delayed flights 2 hours prior to flight hour\n",
    "\n",
    "    # Sort the DataFrame by 'FL_DATE' and 'hour_of_day' in ascending order\n",
    "    df = df.orderBy(\"FL_DATE\", \"hour_of_day\")\n",
    "\n",
    "    # Define a Window specification partitioned by 'FL_DATE' and 'hour_of_day' and ordered by 'FL_DATE' and 'hour_of_day'\n",
    "    window_spec = Window.partitionBy(\"FL_DATE\", \"hour_of_day\",\"ORIGIN\").orderBy(\"FL_DATE\", \"hour_of_day\", \"ORIGIN\")\n",
    "\n",
    "    # Calculate the number of delayed flights in the hour that is 2 hours prior to 'hour_of_day'\n",
    "    df = df.withColumn(\"DELAYED_FLIGHTS_2_HOURS_PRIOR\", F.lag(F.col(\"DEP_DEL15\").cast(\"integer\"), 2).over(window_spec))\n",
    "\n",
    "    # Calculate the total number of flights in the hour that is 2 hours prior to 'hour_of_day'\n",
    "    df = df.withColumn(\"TOTAL_FLIGHTS_2_HOURS_PRIOR\", F.lag(F.lit(1)).over(window_spec))\n",
    "\n",
    "    # Group by 'FL_DATE', 'hour_of_day', and 'departure_airport' and calculate aggregated_data\n",
    "    aggregated_data = df.groupBy(\"FL_DATE\", \"hour_of_day\", \"ORIGIN\").agg(\n",
    "        F.sum(\"DELAYED_FLIGHTS_2_HOURS_PRIOR\").alias(\"DELAYED_FLIGHTS_2_HOURS_PRIOR\"),\n",
    "        F.sum(\"TOTAL_FLIGHTS_2_HOURS_PRIOR\").alias(\"TOTAL_FLIGHTS_2_HOURS_PRIOR\")\n",
    "    )\n",
    "\n",
    "    # Calculate the percentage of delayed flights\n",
    "    aggregated_data = aggregated_data.withColumn(\"DELAYED_PERCENTAGE_2_HOURS_PRIOR\",\n",
    "                                                (F.col(\"DELAYED_FLIGHTS_2_HOURS_PRIOR\") / F.col(\"TOTAL_FLIGHTS_2_HOURS_PRIOR\")) * 100)\n",
    "\n",
    "    # Join the aggregated_data with 'OTPW_12m_small' on 'FL_DATE', 'hour_of_day', and 'departure_airport'\n",
    "    df = df.join(aggregated_data, on=[\"FL_DATE\", \"hour_of_day\", \"ORIGIN\"], how=\"left_outer\")\n",
    "\n",
    "    # Clean up null values:\n",
    "    cols = ['DELAYED_PERCENTAGE_2_HOURS_PRIOR']\n",
    "\n",
    "    # Replace null values in the specified columns with the desired value\n",
    "    for c in cols:\n",
    "        # Replace null values with the value you want (e.g., 0, 'NA', etc.)\n",
    "        df = df.fillna(0, subset=c)  # Replace nulls with 0. You can use any other value you prefer.\n",
    "\n",
    "    # Drop unneeded cols:\n",
    "    columns_to_drop = ['DELAYED_FLIGHTS_2_HOURS_PRIOR', 'TOTAL_FLIGHTS_2_HOURS_PRIOR']\n",
    "\n",
    "    # Drop the specified columns from the DataFrame \n",
    "    df = df.drop(*columns_to_drop)\n",
    "\n",
    "    return df\n",
    "\n",
    "# def add_graph_features(df, save_as=None):\n",
    "#     train_raw = df.filter(col('YEAR') != '2019')\n",
    "\n",
    "#     edges = (train_raw.select('ORIGIN', 'DEST')\n",
    "#             .withColumnRenamed(\"ORIGIN\", \"src\")\n",
    "#             .withColumnRenamed(\"DEST\", \"dst\")).distinct()\n",
    "\n",
    "#     nodes = (edges.distinct().select('src')\n",
    "#             .union(edges.select('dst'))\n",
    "#             .distinct()\n",
    "#             .withColumnRenamed(\"src\", \"id\"))\n",
    "\n",
    "#     graph = GraphFrame(nodes, edges)\n",
    "\n",
    "#     pagerank = spark.read.parquet(f\"{blob_url}/pagerank\")\n",
    "\n",
    "#     in_degrees = graph.inDegrees\n",
    "#     out_degrees = graph.outDegrees\n",
    "#     result = out_degrees.join(in_degrees, out_degrees.id == in_degrees.id, 'left').drop(in_degrees.id)\n",
    "#     result = result.orderBy('id', ascending=True).select('id', 'inDegree', 'outDegree').cache()\n",
    "\n",
    "#     df = OTPW_60M\n",
    "#     df = df.join(result, df.ORIGIN == result.id, 'left').drop(result.id)\n",
    "#     df = df.join(pagerank, df.ORIGIN == pagerank.id, 'left').drop(pagerank.id)\n",
    "\n",
    "#     OTPW_60M.write.option(\"header\", \"true\").parquet(f\"{blob_url}/pipeline_train_js/\")\n",
    "    \n",
    "\n",
    "def one_hot_encode(df):\n",
    "    #list of string variables\n",
    "    categorical_columns = ['FL_DATE','hour_of_day','ORIGIN','QUARTER','DAY_OF_MONTH','DAY_OF_WEEK','YEAR','MONTH','OP_UNIQUE_CARRIER','DEST','origin_type','is_holiday','part_of_day']\n",
    "    #categorical_columns = [c for c, t in df.dtypes if t == 'string' and c != 'DEP_TIME']\n",
    "    # Step 1: StringIndexer - Convert categorical string values to numerical indices\n",
    "    indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\", handleInvalid=\"keep\") for col in categorical_columns]\n",
    "    # Step 2: OneHotEncoder - Convert numerical indices to one-hot encoded vectors\n",
    "    encoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_onehot\") for col in categorical_columns]\n",
    "    # Step 3: Create a pipeline to chain the StringIndexer and OneHotEncoder stages\n",
    "    pipeline = Pipeline(stages=indexers + encoders)\n",
    "    # Fit and transform the pipeline on the DataFrame\n",
    "    encoded_data_df = pipeline.fit(df).transform(df)\n",
    "    # Select the original columns and the newly created one-hot encoded columns\n",
    "    selected_columns = df.columns + [col + \"_onehot\" for col in categorical_columns]\n",
    "    # Select only the required columns from the DataFrame\n",
    "    final_df = encoded_data_df.select(selected_columns)\n",
    "    return final_df\n",
    "\n",
    "def initial_split(df):\n",
    "\n",
    "\n",
    "    # Filter out the data for 2019 as test set\n",
    "    test_data_df = df.filter(\"YEAR = 2019\")\n",
    "    \n",
    "\n",
    "    # Filter the rest of the data as training set\n",
    "    train_data_df = df.filter(\"YEAR != 2019\")\n",
    "\n",
    "    return train_data_df, test_data_df\n",
    "\n",
    "def convert_cat_dataT(df):\n",
    "    # Cast categorical features to string \n",
    "    cat_cols = ['FL_DATE','hour_of_day','ORIGIN','QUARTER','DAY_OF_MONTH','DAY_OF_WEEK','YEAR','MONTH','OP_UNIQUE_CARRIER','DEST','origin_type','is_holiday','part_of_day']\n",
    "    for column_name in cat_cols:\n",
    "        print('columnName to convert:',column_name)\n",
    "        df = df.withColumn(column_name, df[column_name].cast(StringType()))\n",
    "    return df\n",
    "\n",
    "def down_sample(df):\n",
    "    total_count = df.count()\n",
    "    delay_count = df.filter(\"DEP_DEL15 = 1\").count()\n",
    "    split = .5\n",
    "    on_time_count = df.filter(\"DEP_DEL15 = 0\").count()\n",
    "\n",
    "    # Calculate the sampling fraction for the on-time flights\n",
    "    on_time_sampling_fraction = delay_count / on_time_count\n",
    "\n",
    "    # Sample the on-time flights with the calculated sampling fraction\n",
    "    sampled_on_time_flights = df.filter(\"DEP_DEL15 = 0\").sample(withReplacement=False, fraction=on_time_sampling_fraction)\n",
    "\n",
    "    # Union the sampled delayed and on-time flights to create the downsampled DataFrame\n",
    "    downsampled_flight_data = df.filter(\"DEP_DEL15 = 1\").union(sampled_on_time_flights)\n",
    "\n",
    "    # Show the resulting downsampled DataFrame\n",
    "    # display(downsampled_flight_data.limit(100))\n",
    "    return downsampled_flight_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce3297f7-8bb7-433d-9129-00c05306fca7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CustomColumnNormalizer(Transformer):\n",
    "    def __init__(self, input_cols=None, output_col_suffix=\"_normalized\"):\n",
    "        self.input_cols = input_cols\n",
    "        self.output_col_suffix = output_col_suffix\n",
    "        self.vector_assembler = VectorAssembler(inputCols=input_cols, outputCol=\"sfeatures\")\n",
    "        self.scaler = MinMaxScaler(inputCol=\"sfeatures\", outputCol=output_col_suffix)\n",
    "    \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        assembled_data = self.vector_assembler.transform(df)\n",
    "        normalized_data = self.scaler.fit(assembled_data).transform(assembled_data)\n",
    "        \n",
    "        # Get the original columns without the input_cols for renaming\n",
    "        original_columns = [col for col in df.columns if col not in self.input_cols]\n",
    "        \n",
    "        # Create a mapping for renaming the new columns\n",
    "        rename_mapping = {col: col + self.output_col_suffix if col in self.input_cols else col for col in df.columns}\n",
    "        \n",
    "        # Apply alias to DataFrame columns using col() function\n",
    "        normalized_data = normalized_data.select([col(col_name).alias(rename_mapping.get(col_name, col_name)) for col_name in normalized_data.columns])\n",
    "        \n",
    "        # Combine the original columns and the new normalized columns\n",
    "        combined_data = df.select(original_columns).join(normalized_data, on=original_columns, how='inner')\n",
    "        \n",
    "        return combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3efaea1f-7d52-4ff2-a828-aebc95f68305",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Build Pipeline\n",
    "##### -- Pipeline only includes data cleansing and transformations\n",
    "##### -- Cross validation + modeling will be done with the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f892efff-cc87-41b8-b2fc-c783ad0e3976",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create custom function transformers for pipeline\n",
    "num_cols = ['HourlyDewPointTemperature','HourlyDryBulbTemperature','HourlyPrecipitation','HourlyRelativeHumidity','HourlySeaLevelPressure','HourlyVisibility','HourlyWetBulbTemperature','HourlyWindDirection','HourlyWindSpeed','DELAYED_PERCENTAGE_2_HOURS_PRIOR','DISTANCE', 'inDegree', 'outDegree','pagerank']\n",
    "cleanser = CustomFunctionTransformer(data_cleansing)\n",
    "feature_engineer = CustomFunctionTransformer(feature_engineering)\n",
    "ohe = CustomFunctionTransformer(one_hot_encode)\n",
    "handle_nulls = CustomFunctionTransformer(null_handling)\n",
    "cat_dataT = CustomFunctionTransformer(convert_cat_dataT)\n",
    "custom_column_normalizer = CustomColumnNormalizer(input_cols=num_cols, output_col_suffix=\"scaled_features\")\n",
    "train_test_split = CustomFunctionTransformer(initial_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "813ada91-3eed-4ada-a89b-2a9cf8aeab17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columnName to convert: FL_DATE\ncolumnName to convert: hour_of_day\ncolumnName to convert: ORIGIN\ncolumnName to convert: QUARTER\ncolumnName to convert: DAY_OF_MONTH\ncolumnName to convert: DAY_OF_WEEK\ncolumnName to convert: YEAR\ncolumnName to convert: MONTH\ncolumnName to convert: OP_UNIQUE_CARRIER\ncolumnName to convert: DEST\ncolumnName to convert: origin_type\ncolumnName to convert: sched_depart_date_time_UTC\ncolumnName to convert: is_holiday\ncolumnName to convert: part_of_day\n('HourlyDewPointTemperature', 'median')\n('HourlyDryBulbTemperature', 'median')\n('HourlyPrecipitation', 'median')\n('HourlyRelativeHumidity', 'median')\n('HourlySeaLevelPressure', 'median')\n('HourlyVisibility', 'median')\n('HourlyWetBulbTemperature', 'median')\n('HourlyWindDirection', 'median')\n('HourlyWindSpeed', 'median')\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation Pipeline\n",
    "pipeline = Pipeline(stages=[\n",
    "    cleanser,\n",
    "    feature_engineer,\n",
    "    handle_nulls,\n",
    "    train_test_split\n",
    "])\n",
    "\n",
    "# Run pipeline\n",
    "pipeline_model = pipeline.fit(OPT_60M_gf)\n",
    "\n",
    "# Get the transformed data\n",
    "train, test = pipeline_model.transform(OPT_60M_gf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04856437-1aba-4e5f-bbc1-e1e69a81e5c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset date range: Row(min(FL_DATE)='2015-01-01', max(FL_DATE)='2018-12-31')\nTest dataset date range: Row(min(FL_DATE)='2019-01-01', max(FL_DATE)='2019-12-30')\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset date range:\",initial_train.agg(min('FL_DATE'), max('FL_DATE')).first())\n",
    "print(\"Test dataset date range:\",initial_test.agg(min('FL_DATE'), max('FL_DATE')).first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47f625a8-f65a-4c30-aa28-f63acc873261",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 23933364\nTest dataset size: 7263966\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset size:\",initial_train.count())\n",
    "print(\"Test dataset size:\",initial_test.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "621932e9-aec5-4f6a-825a-76d72e59b287",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Pagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ccc4b98-fa9a-4402-8d69-d6b8ed8933ec",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_raw = OTPW_60M.filter(col('YEAR') != '2019')\n",
    "\n",
    "edges = (train_raw.select('ORIGIN', 'DEST')\n",
    "        .withColumnRenamed(\"ORIGIN\", \"src\")\n",
    "        .withColumnRenamed(\"DEST\", \"dst\")).distinct()\n",
    "\n",
    "nodes = (edges.distinct().select('src')\n",
    "        .union(edges.select('dst'))\n",
    "        .distinct()\n",
    "        .withColumnRenamed(\"src\", \"id\"))\n",
    "\n",
    "graph = GraphFrame(nodes, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "925f08cc-c204-4012-a0a1-9514639b0513",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pagerank = spark.read.parquet(f\"{blob_url}/pagerank\")\n",
    "\n",
    "in_degrees = graph.inDegrees\n",
    "out_degrees = graph.outDegrees\n",
    "result = out_degrees.join(in_degrees, out_degrees.id == in_degrees.id, 'left').drop(in_degrees.id)\n",
    "result = result\n",
    "\n",
    "graph_features = pagerank.join(result, pagerank.id == result.id, 'left')\n",
    "graph_features.orderBy('id', ascending=True).select('id', 'inDegree', 'outDegree').cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95996e3b-e69f-40db-b29d-b35317dbe791",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = OTPW_60M\n",
    "df = df.join(result, df.ORIGIN == result.id, 'left').drop(result.id)\n",
    "df = df.join(pagerank, df.ORIGIN == pagerank.id, 'left').drop(pagerank.id).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eecace69-7f4a-43cc-9194-02b31622ac1f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.option(\"header\", \"true\").parquet(f\"{blob_url}/pipeline_train_js/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1594b304-2fc8-4acf-8af9-393ce19b9e3b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "OPT_60M_gf = spark.read.parquet(f\"{blob_url}/pipeline_train_js/\")\n",
    "df = OTPW_60M\n",
    "df = df.join(result, df.ORIGIN == result.id, 'left').drop(result.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dde8a69d-4983-4fc6-bdab-ece518962ec5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# edges\n",
    "g = GraphFrame(nodes, edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74cedf17-f3ba-4778-abd8-6f2975b57a41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "nodes_bro = edges.select(\"ORIGIN\").distinct().union(edges.select(\"DEST\").distinct())\n",
    "origins_df = edges.select(\"ORIGIN\").distinct()\n",
    "destinations_df = edges.select(\"DEST\").distinct()\n",
    "\n",
    "vertices_df = origins_df.union(destinations_df).distinct()\n",
    "\n",
    "edges_df = edges\n",
    "\n",
    "edges_df = edges.withColumnRenamed(\"ORIGIN\", \"src\").withColumnRenamed(\"DEST\", \"dst\")\n",
    "\n",
    "graph = GraphFrame(vertices_df, edges_df)\n",
    "\n",
    "results = graph.pageRank(resetProbability=0.15, maxIter=10)\n",
    "\n",
    "pagerank_scores = results.vertices.select(\"id\", \"pagerank\").orderBy(\"pagerank\", ascending=False)\n",
    "pagerank_scores.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cb304367-c35f-4d7e-a7c7-4ab0fafc06b5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def pageranker(df):\n",
    "origins_df = edges.select(\"ORIGIN\").distinct().withColumnRenamed(\"origin\", \"id\")\n",
    "\n",
    "# Create a DataFrame with unique destinations and rename the 'destination' column to 'id'\n",
    "destinations_df = edges.select(\"DEST\").distinct().withColumnRenamed(\"destination\", \"id\")\n",
    "\n",
    "# Combine origins and destinations to create vertices DataFrame\n",
    "vertices_df = origins_df.union(destinations_df).distinct()\n",
    "\n",
    "# Create edges DataFrame with 'src' and 'dst' columns representing the origin and destination of each flight\n",
    "edges_df = edges.withColumnRenamed(\"origin\", \"src\").withColumnRenamed(\"DEST\", \"dst\")\n",
    "\n",
    "graph = GraphFrame(vertices_df, edges_df)\n",
    "\n",
    "results = graph.pageRank(resetProbability=0.15, maxIter=10)\n",
    "\n",
    "pagerank_scores = results.vertices.select(\"id\", \"pagerank\").orderBy(\"pagerank\", ascending=False)\n",
    "pagerank_scores.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46e8012f-3a04-4c95-a13a-61df54930f23",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def pageranker(df):\n",
    "\n",
    "    df = df.repartition(100) \n",
    "\n",
    "    origins_df = df.select(\"ORIGIN\").distinct().withColumnRenamed(\"origin\", \"id\")\n",
    "\n",
    "    # Create a DataFrame with unique destinations and rename the 'destination' column to 'id'\n",
    "    destinations_df = df.select(\"DEST\").distinct().withColumnRenamed(\"destination\", \"id\")\n",
    "\n",
    "    # Combine origins and destinations to create vertices DataFrame\n",
    "    vertices_df = origins_df.union(destinations_df).distinct()\n",
    "\n",
    "    # Create edges DataFrame with 'src' and 'dst' columns representing the origin and destination of each flight\n",
    "    edges_df = df.withColumnRenamed(\"origin\", \"src\").withColumnRenamed(\"DEST\", \"dst\")\n",
    "\n",
    "    graph = GraphFrame(vertices_df, edges_df)\n",
    "\n",
    "    results = graph.pageRank(resetProbability=0.15, maxIter=10)\n",
    "\n",
    "    pagerank_scores = results.vertices.select(\"id\", \"pagerank\").orderBy(\"pagerank\", ascending=False)\n",
    "    return pagerank_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b650dcd6-3ea8-4508-bcf2-a1052ddd7190",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#### Save or Load pagerank results:\n",
    "# pagerank_60m = test\n",
    "# pagerank_60m.write.option(\"header\", \"true\").parquet(f\"{blob_url}/pagerank/\")\n",
    "pagerank_60m = spark.read.parquet(f\"{blob_url}pagerank/pagerank_60m/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3537a71c-5ab2-4f51-9eab-4528df066ecd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Group by the 'ORIGIN' column\n",
    "grouped_df = df.groupBy(\"ORIGIN\")\\\n",
    "    .agg(\n",
    "        # Collect unique values in 'inDegree' column\n",
    "        F.collect_set(\"inDegree\").alias(\"unique_inDegrees\"),\n",
    "        \n",
    "        # Collect unique values in 'outDegree' column\n",
    "        F.collect_set(\"outDegree\").alias(\"unique_outDegrees\")\n",
    "    )\n",
    "\n",
    "# Show the result\n",
    "display(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9574d8bb-e6ea-429a-8ba9-85fc1b3d8894",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# use the graphframe\n",
    "flights_df = OTPW_60M.limit(500)\n",
    "origins_df = flights_df.select(\"ORIGIN\").distinct()\n",
    "\n",
    "# Create a DataFrame with unique destinations and rename the 'destination' column to 'id'\n",
    "destinations_df = flights_df.select(\"DEST\").distinct()\n",
    "\n",
    "# Combine origins and destinations to create vertices DataFrame\n",
    "vertices_df = origins_df.union(destinations_df).distinct()\n",
    "\n",
    "# Create edges DataFrame with 'src' and 'dst' columns representing the origin and destination of each flight\n",
    "edges_df = flights_df.withColumnRenamed(\"origin\", \"src\").withColumnRenamed(\"DEST\", \"dst\")\n",
    "\n",
    "graph = GraphFrame(vertices_df, edges_df)\n",
    "\n",
    "\n",
    "flights_df = OTPW_60M.limit(100)\n",
    "vertices_df = graph.vertices\n",
    "edges_df = graph.edges\n",
    "in_degrees = graph.inDegrees\n",
    "out_degrees = graph.outDegrees\n",
    "\n",
    "\n",
    "other_columns = []\n",
    "result_df = flights_df.join(edges_df, (flights_df.origin == edges_df.src) & (flights_df.destination == edges_df.dst))\n",
    "result_df = result_df.join(in_degrees, result_df.origin == in_degrees.id, 'left_outer').withColumnRenamed(\"inDegree\", \"origin_in_degree\")\n",
    "result_df = result_df.join(out_degrees, result_df.origin == out_degrees.id, 'left_outer').withColumnRenamed(\"outDegree\", \"origin_out_degree\")\n",
    "result_df = result_df.select(\"origin\", \"destination\", \"origin_in_degree\", \"origin_out_degree\", *other_columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2191510-0227-4983-ae51-909b80242c2b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Normalization of Data\n",
    "#### Normalize Test Data, Hold Off On Normalizing Train/Validation Data until SMOTE function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b594a3ef-ded2-42b5-88e5-3e204cf990f2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CustomColumnNormalizer(Transformer):\n",
    "    def __init__(self, input_cols=None, scaler_model=None, output_col_suffix=\"_normalized\"):\n",
    "        self.input_cols = input_cols\n",
    "        self.output_col_suffix = output_col_suffix\n",
    "        self.vector_assembler = VectorAssembler(inputCols=input_cols, outputCol=\"num_features\")\n",
    "        self.scaler_model = scaler_model\n",
    "    \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        assembled_data = self.vector_assembler.transform(df)\n",
    "        normalized_data = self.scaler_model.transform(assembled_data)\n",
    "\n",
    "        return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d01c1935-14a1-46af-9f06-7a175369b937",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columnName to convert: FL_DATE\ncolumnName to convert: hour_of_day\ncolumnName to convert: ORIGIN\ncolumnName to convert: QUARTER\ncolumnName to convert: DAY_OF_MONTH\ncolumnName to convert: DAY_OF_WEEK\ncolumnName to convert: YEAR\ncolumnName to convert: MONTH\ncolumnName to convert: OP_UNIQUE_CARRIER\ncolumnName to convert: DEST\ncolumnName to convert: origin_type\ncolumnName to convert: sched_depart_date_time_UTC\ncolumnName to convert: is_holiday\ncolumnName to convert: part_of_day\ncategorical_columns ['FL_DATE', 'hour_of_day', 'ORIGIN', 'QUARTER', 'DAY_OF_MONTH', 'DAY_OF_WEEK', 'YEAR', 'MONTH', 'OP_UNIQUE_CARRIER', 'DEST', 'origin_type', 'sched_depart_date_time_UTC', 'is_holiday', 'part_of_day']\ndtypes: [('FL_DATE', 'string'), ('hour_of_day', 'string'), ('ORIGIN', 'string'), ('QUARTER', 'string'), ('DAY_OF_MONTH', 'string'), ('DAY_OF_WEEK', 'string'), ('YEAR', 'string'), ('MONTH', 'string'), ('OP_UNIQUE_CARRIER', 'string'), ('DEST', 'string'), ('DEP_TIME', 'string'), ('DEP_DEL15', 'float'), ('DISTANCE', 'float'), ('origin_type', 'string'), ('sched_depart_date_time_UTC', 'string'), ('HourlyDewPointTemperature', 'double'), ('HourlyDryBulbTemperature', 'double'), ('HourlyPrecipitation', 'double'), ('HourlyRelativeHumidity', 'double'), ('HourlySeaLevelPressure', 'double'), ('HourlyVisibility', 'double'), ('HourlyWetBulbTemperature', 'double'), ('HourlyWindDirection', 'double'), ('HourlyWindSpeed', 'double'), ('is_holiday', 'string'), ('part_of_day', 'string'), ('DELAYED_PERCENTAGE_2_HOURS_PRIOR', 'double'), ('num_features', 'vector'), ('scaled_features', 'vector')]\nindexed\nencoders\ncreate pipeline\nfit & transform pipeline\nselect cols\n"
     ]
    }
   ],
   "source": [
    "# Create Custom Transformer To Normalize All Datasets (Train/Validation/Test)\n",
    "class CustomColumnNormalizer(Transformer):\n",
    "    def __init__(self, input_cols=None, scaler_model=None, output_col_suffix=\"_normalized\"):\n",
    "        self.input_cols = input_cols\n",
    "        self.output_col_suffix = output_col_suffix\n",
    "        self.vector_assembler = VectorAssembler(inputCols=input_cols, outputCol=\"num_features\")\n",
    "        self.scaler_model = scaler_model\n",
    "    \n",
    "    def _transform(self, df: DataFrame) -> DataFrame:\n",
    "        assembled_data = self.vector_assembler.transform(df)\n",
    "        normalized_data = self.scaler_model.transform(assembled_data)\n",
    "\n",
    "        return normalized_data\n",
    "\n",
    "# Create Scaler Model Using Training data\n",
    "num_cols = ['HourlyDewPointTemperature','HourlyDryBulbTemperature','HourlyPrecipitation','HourlyRelativeHumidity','HourlySeaLevelPressure','HourlyVisibility','HourlyWetBulbTemperature','HourlyWindDirection','HourlyWindSpeed','DELAYED_PERCENTAGE_2_HOURS_PRIOR','DISTANCE']\n",
    "cat_cols = ['FL_DATE','hour_of_day','ORIGIN','QUARTER','DAY_OF_MONTH','DAY_OF_WEEK','YEAR','MONTH','OP_UNIQUE_CARRIER','DEST','origin_type','sched_depart_date_time_UTC','is_holiday','part_of_day']\n",
    "num_assembler = VectorAssembler(inputCols = num_cols, outputCol = 'num_features')\n",
    "num_assembled_train = num_assembler.transform(initial_train)\n",
    "train_scaler = MinMaxScaler(inputCol='num_features', outputCol='scaled_features').fit(num_assembled_train)\n",
    "custom_column_normalizer = CustomColumnNormalizer(input_cols=num_cols, scaler_model=train_scaler, output_col_suffix=\"scaled_features\")\n",
    "\n",
    "# Scale and OHE Test Data\n",
    "pipeline_test = Pipeline(stages=[custom_column_normalizer,cat_dataT,ohe])\n",
    "test_vectorized = pipeline_test.fit(initial_test).transform(initial_test)\n",
    "test_vectorized = test_vectorized.withColumnRenamed('DEP_DEL15', 'label').drop('num_features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7735a7b-97d2-494f-8546-c14890235e19",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Cross Validation Split On Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1975ac0-51e7-4b0a-95d0-c526e8515c1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "BlockSplit = namedtuple(\"BlockSplit\", [\"min_date\", \"train_cut\", \"val_cut\"])\n",
    "\n",
    "def make_block_splits(min_date, max_date, train_width, val_width):\n",
    "    \"\"\"\n",
    "    Return a list of breakpoints for building the train and validation folds.\n",
    "\n",
    "    Each item in the list contains a namedtuple of dates: (min_date, train_cut, val_cut).\n",
    "    The folds can be generated using these cuts as follows: \n",
    "    \n",
    "    * train data is (min_date <= flight_date < train_cut)\n",
    "    * val data is (train_cut <= flight_date < val_cut)\n",
    "\n",
    "    Args:\n",
    "        min_date - the first date to include in the first fold\n",
    "        max_date - the validation cut of the last fold (i.e. this is the first day\n",
    "            of the test set and is not included in the folds)\n",
    "        train_width - the timespan covered by each training set\n",
    "        val_delta - the timespan covered by each validation set\n",
    "\n",
    "    Returns:\n",
    "        a list of ``BlockSplit`` instances\n",
    "    \"\"\"\n",
    "    blocks = list()\n",
    "    train_min = min_date\n",
    "    val_cut = min_date\n",
    "    \n",
    "    # Loop over and create blocks\n",
    "    while True:\n",
    "        train_cut = train_min + train_width\n",
    "        val_cut = train_cut + val_width\n",
    "        if train_cut > max_date or val_cut > max_date:\n",
    "            break\n",
    "        blocks.append(BlockSplit(train_min, train_cut, val_cut))\n",
    "        train_min = val_cut\n",
    "    \n",
    "    return blocks\n",
    "\n",
    "def make_folds(df, date_col, splits):\n",
    "    \"\"\"\n",
    "    Make folds using the specified block splits.\n",
    "\n",
    "    Args:\n",
    "        df - the dataframe to make folds from\n",
    "        date_col - the name of a date column to split on\n",
    "        splits - a list of ``BlockSplit`` instances\n",
    "    \n",
    "    Returns:\n",
    "        a list of (train_df, val_df) tuples\n",
    "    \"\"\"\n",
    "    folds = list()\n",
    "    count = 0\n",
    "    for split in splits:\n",
    "        print(\"FOLD\",count)\n",
    "        train_df = df.filter((df[date_col] >= split.min_date) & (df[date_col] < split.train_cut))\n",
    "        train_df = down_sample(train_df)\n",
    "        val_df = df.filter((df[date_col] >= split.train_cut) & (df[date_col] < split.val_cut))\n",
    "        folds.append((train_df, val_df))\n",
    "        count += 1\n",
    "    return folds\n",
    "\n",
    "def save_folds_to_blob(folds, blob_url, fold_name):\n",
    "    for i, (train_df, val_df) in enumerate(folds):\n",
    "        dbutils.fs.rm(f\"{blob_url}/{fold_name}/train_{i}_df\", recurse=True)\n",
    "        dbutils.fs.rm(f\"{blob_url}/{fold_name}/val_{i}_df\", recurse=True)\n",
    "        train_df.write.option(\"header\", \"true\").parquet(f\"{blob_url}/{fold_name}/train_{i}_df\")\n",
    "        val_df.write.option(\"header\", \"true\").parquet(f\"{blob_url}/{fold_name}/val_{i}_df\")\n",
    "\n",
    "def make_folds_and_save(df_in, splits, fold_name=\"folds\"):\n",
    "    \"\"\"\n",
    "    Create training folds and save to blob storage.\n",
    "\n",
    "    Args:\n",
    "        df_in - the dataframe to split into folds\n",
    "        splits - the block splits as returned by make_block_splits()\n",
    "    \"\"\"\n",
    "    if input(f\"Are you sure you want to overwrite '{fold_name}'? (y/n)\").lower().strip() == \"y\":\n",
    "        print(\"Making train/val folds...\")\n",
    "        df = df_in\n",
    "        folds = make_folds(df, \"FL_DATE\", splits)\n",
    "        save_folds_to_blob(folds, blob_url, fold_name)\n",
    "    else:\n",
    "        print(\"Cancelled\")\n",
    "\n",
    "def save_df(df_in):\n",
    "    \"\"\"\n",
    "    Create training folds and save to blob storage.\n",
    "\n",
    "    Args:\n",
    "        df_in - the dataframe to split into folds\n",
    "        splits - the block splits as returned by make_block_splits()\n",
    "    \"\"\"\n",
    "    if input(f\"Are you sure you want to overwrite '{fold_name}'? (y/n)\").lower().strip() == \"y\":\n",
    "        print(\"Making train/val folds...\")\n",
    "        df = df_in\n",
    "        folds = make_folds(df, \"FL_DATE\", splits)\n",
    "        save_folds_to_blob(folds, blob_url, fold_name)\n",
    "    else:\n",
    "        print(\"Cancelled\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4e6493f-6860-4eba-a199-a61b9d4fe0cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbutils.fs.rm(f\"wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/\", recurse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5c61447-0d52-4f7c-a896-b706f313f35b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0 Splits:\n  Train: 2015-01-01 <= x < 2015-09-08\n    Val: 2015-09-08 <= x < 2015-10-18\nFold 1 Splits:\n  Train: 2015-10-18 <= x < 2016-06-24\n    Val: 2016-06-24 <= x < 2016-08-03\nFold 2 Splits:\n  Train: 2016-08-03 <= x < 2017-04-10\n    Val: 2017-04-10 <= x < 2017-05-20\nFold 3 Splits:\n  Train: 2017-05-20 <= x < 2018-01-25\n    Val: 2018-01-25 <= x < 2018-03-06\nFold 4 Splits:\n  Train: 2018-03-06 <= x < 2018-11-11\n    Val: 2018-11-11 <= x < 2018-12-21\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Are you sure you want to overwrite 'temp_folds'? (y/n) y"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making train/val folds...\nFOLD 0\nFOLD 1\nFOLD 2\nFOLD 3\nFOLD 4\n"
     ]
    }
   ],
   "source": [
    "blob_container  = \"w261-container\"\n",
    "storage_account = \"whiskeythedog\"\n",
    "secret_scope    = \"whiskeythedog-scope\"                   \n",
    "secret_key      = \"whikeythedog-key\"  \n",
    "blob_url = f\"wasbs://{blob_container}@{storage_account}.blob.core.windows.net\"\n",
    "\n",
    "spark.conf.set(\n",
    "  f\"fs.azure.sas.{blob_container}.{storage_account}.blob.core.windows.net\",\n",
    "  dbutils.secrets.get(scope = secret_scope, key = secret_key)\n",
    ")\n",
    "\n",
    "# Compute the split points. (This does not actually create the folds)\n",
    "splits = make_block_splits(\n",
    "    min_date = date(2015,1,1), \n",
    "    max_date = date(2018,12,31), \n",
    "    train_width = timedelta(days=250), \n",
    "    val_width = timedelta(days=40),\n",
    ")\n",
    "\n",
    "for i, split in enumerate(splits):\n",
    "    print(f\"Fold {i} Splits:\")\n",
    "    print(f\"  Train: {split.min_date} <= x < {split.train_cut}\")\n",
    "    print(f\"    Val: {split.train_cut} <= x < {split.val_cut}\")\n",
    "\n",
    "make_folds_and_save(initial_train, splits, fold_name=\"temp_folds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1473a31-3620-4e9c-80f6-ebb04ddd4c36",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/train_0_df/</td><td>train_0_df/</td><td>0</td><td>1691353388000</td></tr><tr><td>wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/train_1_df/</td><td>train_1_df/</td><td>0</td><td>1691353543000</td></tr><tr><td>wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/train_2_df/</td><td>train_2_df/</td><td>0</td><td>1691353627000</td></tr><tr><td>wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/train_3_df/</td><td>train_3_df/</td><td>0</td><td>1691353696000</td></tr><tr><td>wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/train_4_df/</td><td>train_4_df/</td><td>0</td><td>1691353767000</td></tr><tr><td>wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/val_0_df/</td><td>val_0_df/</td><td>0</td><td>1691353479000</td></tr><tr><td>wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/val_1_df/</td><td>val_1_df/</td><td>0</td><td>1691353589000</td></tr><tr><td>wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/val_2_df/</td><td>val_2_df/</td><td>0</td><td>1691353656000</td></tr><tr><td>wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/val_3_df/</td><td>val_3_df/</td><td>0</td><td>1691353729000</td></tr><tr><td>wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/val_4_df/</td><td>val_4_df/</td><td>0</td><td>1691353801000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/train_0_df/",
         "train_0_df/",
         0,
         1691353388000
        ],
        [
         "wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/train_1_df/",
         "train_1_df/",
         0,
         1691353543000
        ],
        [
         "wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/train_2_df/",
         "train_2_df/",
         0,
         1691353627000
        ],
        [
         "wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/train_3_df/",
         "train_3_df/",
         0,
         1691353696000
        ],
        [
         "wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/train_4_df/",
         "train_4_df/",
         0,
         1691353767000
        ],
        [
         "wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/val_0_df/",
         "val_0_df/",
         0,
         1691353479000
        ],
        [
         "wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/val_1_df/",
         "val_1_df/",
         0,
         1691353589000
        ],
        [
         "wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/val_2_df/",
         "val_2_df/",
         0,
         1691353656000
        ],
        [
         "wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/val_3_df/",
         "val_3_df/",
         0,
         1691353729000
        ],
        [
         "wasbs://w261-container@whiskeythedog.blob.core.windows.net/temp_folds/val_4_df/",
         "val_4_df/",
         0,
         1691353801000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(f\"{blob_url}/temp_folds/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d7acb6a8-09ef-46f5-a5fc-697ce25cc03c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "784f5025-19ad-4b92-8bba-95f35439f905",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def pre_smote_df_process(df,num_cols,cat_cols,target_col,index_suffix=\"_index\"):\n",
    "    '''\n",
    "    string indexer and vector assembler\n",
    "    inputs:\n",
    "    * df: spark df, original\n",
    "    * num_cols: numerical cols to be assembled\n",
    "    * cat_cols: categorical cols to be stringindexed\n",
    "    * target_col: prediction target\n",
    "    * index_suffix: will be the suffix after string indexing\n",
    "    output:\n",
    "    * vectorized: spark df, after stringindex and vector assemble, ready for smote\n",
    "    '''\n",
    "    print('pre smote process')\n",
    "    if(df.select(target_col).distinct().count() != 2):\n",
    "        raise ValueError(\"Target col must have exactly 2 classes\")\n",
    "\n",
    "    if target_col in num_cols:\n",
    "        num_cols.remove(target_col)\n",
    "\n",
    "    # window = Window.orderBy(F.monotonically_increasing_id())\n",
    "    # df = df.withColumn(\"index\", F.row_number().over(window))  \n",
    "\n",
    "    # # Remove DEP_TIME column (will not be part of feature set)\n",
    "    # # To be reinserted later\n",
    "    # removed_dep_time = \"DEP_TIME\"\n",
    "    # removed_dep_time_values = df.select(\"index\", removed_dep_time)\n",
    "    # df = df.drop('DEP_TIME')\n",
    "\n",
    "    # # Remove DEP_DEL15 label (will not be part of feature set)\n",
    "    # # To be reinserted later\n",
    "    # removed_dep_del15 = \"DEP_DEL15\"\n",
    "    # removed_dep_del15_values = df.select(\"index\", removed_dep_del15)\n",
    "    # df = df.drop('DEP_DEL15')\n",
    "\n",
    "    # index the string cols, except possibly for the label col\n",
    "    # index the string cols, except possibly for the label col \n",
    "    cat_indexers = [(column, StringIndexer(inputCol=column, outputCol=column+index_suffix, handleInvalid='keep').fit(df)) for column in list(set(cat_cols)-set([target_col]))]\n",
    "    assemble_stages = [x[1] for x in cat_indexers]\n",
    "    # add the stage of numerical vector assembler\n",
    "    assembler = VectorAssembler(inputCols = num_cols, outputCol = 'features')\n",
    "    #assemble_stages.append(custom_column_normalizer)\n",
    "    assemble_stages.append(assembler)\n",
    "    pipeline = Pipeline(stages=assemble_stages)\n",
    "    pos_vectorized = pipeline.fit(df).transform(df)\n",
    "\n",
    "    # # add the stage of numerical vector assembler\n",
    "    # assemble_stages.append(custom_column_normalizer)\n",
    "    # pipeline = Pipeline(stages=assemble_stages)\n",
    "    # pos_vectorized = pipeline.fit(df).transform(df)\n",
    "\n",
    "    # drop original num cols and cat cols\n",
    "    drop_cols = num_cols+cat_cols\n",
    "    \n",
    "    keep_cols = [a for a in pos_vectorized.columns if a not in drop_cols]\n",
    "    \n",
    "    vectorized = pos_vectorized.select(*keep_cols).withColumn('label',pos_vectorized[target_col]).drop(target_col)\n",
    "    \n",
    "    return vectorized, cat_indexers\n",
    "\n",
    "def smote(vectorized_sdf,smote_config):\n",
    "    '''\n",
    "    contains logic to perform smote oversampling, given a spark df with 2 classes\n",
    "    inputs:\n",
    "    * vectorized_sdf: cat cols are already stringindexed, num cols are assembled into 'features' vector\n",
    "      df target col should be 'label'\n",
    "    * smote_config: config obj containing smote parameters\n",
    "    output:\n",
    "    * oversampled_df: spark df after smote oversampling\n",
    "    '''\n",
    "    dataInput_min = vectorized_sdf[vectorized_sdf['label'] == 1]\n",
    "    dataInput_maj = vectorized_sdf[vectorized_sdf['label'] == 0]\n",
    "    \n",
    "    # LSH, bucketed random projection\n",
    "    brp = BucketedRandomProjectionLSH(inputCol=\"features\", outputCol=\"hashes\",seed=smote_config['seed'], bucketLength=smote_config['bucketLength'])\n",
    "    # smote only applies on existing minority instances    \n",
    "    model = brp.fit(dataInput_min)\n",
    "    model.transform(dataInput_min)\n",
    "\n",
    "    # here distance is calculated from brp's param inputCol\n",
    "    self_join_w_distance = model.approxSimilarityJoin(dataInput_min, dataInput_min, float(\"inf\"), distCol=\"EuclideanDistance\")\n",
    "\n",
    "    # remove self-comparison (distance 0)\n",
    "    self_join_w_distance = self_join_w_distance.filter(self_join_w_distance.EuclideanDistance > 0)\n",
    "\n",
    "    over_original_rows = Window.partitionBy(\"datasetA\").orderBy(\"EuclideanDistance\")\n",
    "\n",
    "    self_similarity_df = self_join_w_distance.withColumn(\"r_num\", F.row_number().over(over_original_rows))\n",
    "\n",
    "    self_similarity_df_selected = self_similarity_df.filter(self_similarity_df.r_num <= smote_config['k'])\n",
    "\n",
    "    over_original_rows_no_order = Window.partitionBy('datasetA')\n",
    "\n",
    "    # list to store batches of synthetic data\n",
    "    res = []\n",
    "    \n",
    "    # two udf for vector add and subtract, subtraction include a random factor [0,1]\n",
    "    subtract_vector_udf = F.udf(lambda arr: random.uniform(0, 1)*(arr[0]-arr[1]), VectorUDT())\n",
    "    add_vector_udf = F.udf(lambda arr: arr[0]+arr[1], VectorUDT())\n",
    "    \n",
    "    # retain original columns\n",
    "    original_cols = dataInput_min.columns\n",
    "    \n",
    "    for i in range(smote_config['multiplier']):\n",
    "        print(\"generating batch %s of synthetic instances\"%i)\n",
    "        # logic to randomly select neighbour: pick the largest random number generated row as the neighbour\n",
    "        df_random_sel = self_similarity_df_selected.withColumn(\"rand\", F.rand()).withColumn('max_rand', F.max('rand').over(over_original_rows_no_order))\\\n",
    "                            .where(F.col('rand') == F.col('max_rand')).drop(*['max_rand','rand','r_num'])\n",
    "        # create synthetic feature numerical part\n",
    "        df_vec_diff = df_random_sel.select('*', subtract_vector_udf(F.array('datasetA.features', 'datasetB.features')).alias('vec_diff'))\n",
    "        df_vec_modified = df_vec_diff.select('*', add_vector_udf(F.array('datasetA.features', 'vec_diff')).alias('features'))\n",
    "        \n",
    "        # for categorical cols, either pick original or the neighbour's cat values\n",
    "        for c in original_cols:\n",
    "            # randomly select neighbour or original data\n",
    "            col_sub = random.choice(['datasetA','datasetB'])\n",
    "            val = \"{0}.{1}\".format(col_sub,c)\n",
    "            if c != 'features':\n",
    "                # do not unpack original numerical features\n",
    "                df_vec_modified = df_vec_modified.withColumn(c,F.col(val))\n",
    "        \n",
    "        # this df_vec_modified is the synthetic minority instances,\n",
    "        df_vec_modified = df_vec_modified.drop(*['datasetA','datasetB','vec_diff','EuclideanDistance'])\n",
    "        \n",
    "        res.append(df_vec_modified)\n",
    "    \n",
    "    dfunion = reduce(DataFrame.unionAll, res)\n",
    "    # union synthetic instances with original full (both minority and majority) df\n",
    "    oversampled_df = dfunion.union(vectorized_sdf.select(dfunion.columns))\n",
    "    \n",
    "    return oversampled_df\n",
    "def reverse_string_index(df, indexers):\n",
    "    print('reverse string index')\n",
    "\n",
    "    for i in indexers:\n",
    "        current_col = i[0] + '_smoteIndex'\n",
    "        og_col = i[0]\n",
    "        string_indexer_model = i[1]\n",
    "        mapping = dict(enumerate(string_indexer_model.labels))\n",
    "        index_to_string = IndexToString(inputCol=current_col, outputCol=og_col, labels=list(mapping.values()))\n",
    "        # Apply the transformer on the indexed DataFrame\n",
    "        df = index_to_string.transform(df).drop(current_col)\n",
    "\n",
    "    return df\n",
    "\n",
    "def train_smote(train):\n",
    "    smote_vectorized, cat_indexers = pre_smote_df_process(train,num_cols,cat_cols,'DEP_DEL15',index_suffix=\"_smoteIndex\")\n",
    "    smote_config = {'seed':42, 'bucketLength':10, 'k':5, 'multiplier':15}\n",
    "    smote_result = smote(smote_vectorized,smote_config)\n",
    "    reversed_df = reverse_string_index(smote_result, cat_indexers)\n",
    "    return reversed_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61088930-3a76-4cfc-8b4e-0c7b92f4abb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_folds_from_blob_and_cache(blob_url, fold_name):\n",
    "    folds = list()\n",
    "\n",
    "    # Compute the fold count\n",
    "    files = dbutils.fs.ls(f\"{blob_url}/{fold_name}\")\n",
    "    fold_names = sorted([f.name for f in files if f.name.startswith(\"train\")])\n",
    "    match = re.match(r\"train_(\\d+)_df\", fold_names[-1])\n",
    "    fold_count = int(match.group(1)) + 1\n",
    "    print(f\"Loading {fold_count} folds...\")\n",
    "\n",
    "    # Load folds\n",
    "    for i in range(fold_count):\n",
    "        train_df = (\n",
    "            spark.read.parquet(f\"{blob_url}/{fold_name}/train_{i}_df\")\n",
    "            .cache()\n",
    "        )\n",
    "        val_df = (\n",
    "            spark.read.parquet(f\"{blob_url}/{fold_name}/val_{i}_df\")\n",
    "            .cache()\n",
    "        )\n",
    "        folds.append((i, train_df, val_df))\n",
    "    return folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "295c2ecb-fbe5-4334-b016-bacf6d924ad9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "    val_i_df = spark.read.parquet(f\"{blob_url}/temp_folds/val_{i}_df\")\n",
    "    print('val_i_df count:',val_i_df.count())\n",
    "    print('val_i_df cols',val_i_df.dtypes)\n",
    "    pipeline_val = Pipeline(stages=[custom_column_normalizer,cat_dataT])\n",
    "    print('pipeline val')\n",
    "    val_i_vectorized = pipeline_val.fit(val_i_df).transform(val_i_df)\n",
    "    print('fit+transform val')\n",
    "    val_i_vectorized = val_i_vectorized.withColumnRenamed('DEP_DEL15', 'label').drop('num_features')\n",
    "    # val_i_vectorized = one_hot_encode(val_i_vectorized)\n",
    "    print('format val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdb5e91a-3a1b-4314-be34-b7dfa258dd3f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\ntrain_i_df count: 3952323\n<class 'pyspark.sql.dataframe.DataFrame'>\ntrain_icols [('FL_DATE', 'string'), ('hour_of_day', 'int'), ('ORIGIN', 'string'), ('QUARTER', 'string'), ('DAY_OF_MONTH', 'string'), ('DAY_OF_WEEK', 'string'), ('YEAR', 'string'), ('MONTH', 'string'), ('OP_UNIQUE_CARRIER', 'string'), ('DEST', 'string'), ('DEP_TIME', 'string'), ('DEP_DEL15', 'float'), ('DISTANCE', 'float'), ('origin_type', 'string'), ('sched_depart_date_time_UTC', 'string'), ('HourlyDewPointTemperature', 'double'), ('HourlyDryBulbTemperature', 'double'), ('HourlyPrecipitation', 'double'), ('HourlyRelativeHumidity', 'double'), ('HourlySeaLevelPressure', 'double'), ('HourlyVisibility', 'double'), ('HourlyWetBulbTemperature', 'double'), ('HourlyWindDirection', 'double'), ('HourlyWindSpeed', 'double'), ('is_holiday', 'int'), ('part_of_day', 'int'), ('DELAYED_PERCENTAGE_2_HOURS_PRIOR', 'double')]\npre smote process\ngenerating batch 0 of synthetic instances\ngenerating batch 1 of synthetic instances\ngenerating batch 2 of synthetic instances\ngenerating batch 3 of synthetic instances\ngenerating batch 4 of synthetic instances\ngenerating batch 5 of synthetic instances\ngenerating batch 6 of synthetic instances\ngenerating batch 7 of synthetic instances\ngenerating batch 8 of synthetic instances\ngenerating batch 9 of synthetic instances\ngenerating batch 10 of synthetic instances\ngenerating batch 11 of synthetic instances\ngenerating batch 12 of synthetic instances\ngenerating batch 13 of synthetic instances\ngenerating batch 14 of synthetic instances\nreverse string index\nSMOTE FINISHED!\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_folds = []\n",
    "num_folds = int(len(dbutils.fs.ls(f\"{blob_url}/temp_folds/\"))/2)\n",
    "\n",
    "for i in range(num_folds): \n",
    "    print(\"FOLD\",i)\n",
    "\n",
    "    train_i_df = spark.read.parquet(f\"{blob_url}/temp_folds/train_{i}_df\").cache()\n",
    "    print('train_i_df count:',train_i_df.count())\n",
    "    print(type(train_i_df))\n",
    "    print('train_icols',train_i_df.dtypes)\n",
    "    smote_train_i_df = train_smote(train_i_df)\n",
    "    print('SMOTE FINISHED!')\n",
    "    print('SMOTE train_i_df count:',smote_train_i_df.count())\n",
    "    print('SMOTE train_i_df cols',smote_train_i_df.dtypes)\n",
    "    train_folds.append((i, smote_train_i_df))\n",
    "    print(\"*\" * 100)\n",
    "    print(\"\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Final W261 Notebook",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
